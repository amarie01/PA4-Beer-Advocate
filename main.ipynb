{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import string\n",
    "import operator\n",
    "import time\n",
    "\n",
    "from models import *\n",
    "from config import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as torch_init\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from nltk.translate import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = torch.load('LSTM_model2_epoch15.pt')\n",
    "mod.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# -------------------\n",
    "#   Reads from the csv file given by 'fname' and returns a \n",
    "#   pandas DataFrame of the read csv\n",
    "def load_data(fname):\n",
    "    return pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# -------------------\n",
    "#   Converts each character in 'review_txt' string to a hot-encoded value. \n",
    "#   \n",
    "#   ** NOTE **\n",
    "#\n",
    "#   All letters are converted to lowercase, and all special characters \n",
    "#   (except for <space>, <period>, <comma>, <semicolon>, <dash>, <parentheses> \n",
    "#   <forward slash>) are ignored\n",
    "def text_to_onehot(review_txt):\n",
    "    encoded = []\n",
    "    \n",
    "    for char in review_txt:\n",
    "        c = np.zeros(n_letters)\n",
    "        char = char.lower()\n",
    "\n",
    "        if char in all_letters:\n",
    "            c[all_letters.find(char) + 1] = 1  # Shift right 1 for SOS char\n",
    "            encoded.append(c)\n",
    "    \n",
    "    # Return an numpy array\n",
    "    return np.array(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# -------------------\n",
    "#   Converts each array in 'onehot' numpy array to a its corresponding \n",
    "#   character: either a lowercase letter, a digit, a <space>, a <period>, \n",
    "#   a <comma>, a <semicolon>, a <dash>, a <parentheses>, a <forward slash>, \n",
    "#   or a padding character (<SOS> or <EOS>) \n",
    "#   \n",
    "#   ** NOTE **\n",
    "#\n",
    "#  'onehot' is a numpy array of numpy arrays, where each subarray represents\n",
    "#  a one-hot encoding of a single character in the review text.\n",
    "def onehot_to_text(onehot):\n",
    "    text = \"\"\n",
    "    \n",
    "    for encoded_char in onehot:\n",
    "        i = encoded_char.max(0)[1] # argmax\n",
    "        \n",
    "        if i == 0:\n",
    "            text += (\"<SOS>\")\n",
    "        elif i == (n_letters - 1):\n",
    "            text += (\"<EOS>\")\n",
    "        elif i == (n_letters - 2):\n",
    "            text += (\"<PAD>\")\n",
    "        else:\n",
    "            text += (all_letters[i - 1])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_sequence(X_batch, y_batch):\n",
    "    sequence = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(len(y_batch)):\n",
    "        review = []\n",
    "        label = []\n",
    "        \n",
    "        for char in y_batch[i]:\n",
    "            style = X_batch[i][0]\n",
    "            rating = X_batch[i][1]\n",
    "            \n",
    "            char_index = char.argmax(axis=0)\n",
    "            \n",
    "            if char_index != 0:\n",
    "                label.append(char.argmax(axis=0))     \n",
    "            \n",
    "            review.append(torch.from_numpy(\n",
    "                np.concatenate((style, np.array([rating]), char)))) \n",
    "          \n",
    "        labels.append(torch.from_numpy(np.array(label)))\n",
    "        \n",
    "        review = torch.stack(review,dim=0)\n",
    "        sequence.append(review)\n",
    "        \n",
    "    sequence = torch.stack(sequence,dim=0)\n",
    "    labels = torch.stack(labels,dim=0)\n",
    "    \n",
    "    return sequence, labels\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_BLEUscore(reference, hypothesis):\n",
    "    references = re.compile('\\w+').findall(reference)\n",
    "    hypotheses = re.compile('\\w+').findall(hypothesis)\n",
    "    \n",
    "    # 1-gram score\n",
    "    return bleu_score.sentence_bleu([references], hypotheses, weights=(1, 0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# -------------------\n",
    "#   Input 'data' is a pandas DataFrame. This func returns \n",
    "#   a numpy array that has all features (including all\n",
    "#   text characters in one hot encoded form).\n",
    "def process_train_data(data):\n",
    "    unique_styles = list(beer_styles.keys())\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over pandas DataFrame\n",
    "    for i, review in enumerate(data.iterrows()):\n",
    "        review = review[1]\n",
    "        \n",
    "        text = review['review/text']\n",
    "        style = review['beer/style']\n",
    "        rating = review['review/overall']\n",
    "        \n",
    "        onehot_style = np.zeros(len(unique_styles))   # Initialize one-hot encoded\n",
    "        onehot_style[unique_styles.index(style)] = 1  # One-hot encode beer style\n",
    "        \n",
    "        print(\"---Process Training---Index: \" + str(i) + \"; Percent complete: \" + \n",
    "              str(round((i/ (1.0 * len(data)) * 100), 2)) + \"%\")\n",
    "        \n",
    "        text = text.replace(\"!\", \".\")    # Set all ending punctuation to \".\"\n",
    "        text = text.replace(\"?\", \".\")    # Set all ending punctuation to \".\"\n",
    "        text = re.sub('\\s+', ' ', text)  # Remove \\n and \\t\n",
    "        text = text.lower()              # Remove all uppercase letters\n",
    "        \n",
    "        features.append(np.array([onehot_style, rating]))\n",
    "        labels.append(text)\n",
    "        \n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# -------------------\n",
    "#   Takes in training data and labels as numpy array and applies a 90-10 split\n",
    "#   for the training and validation data, respectively. Returns a set of \n",
    "#   4 numpy arrays, where each corresponds to the train_data, train_labels, \n",
    "#   valid_data, and valid_labels.\n",
    "def train_valid_split(data, labels):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_valid = []\n",
    "    y_valid = []\n",
    "    \n",
    "    val_split = int(0.1 * labels.size)  # 10% of data allocated to validation set\n",
    "    \n",
    "    all_indices = np.indices(labels.shape)[0]\n",
    "    val_indices = np.random.choice(all_indices, val_split, replace=False)\n",
    "    \n",
    "    for i in range(labels.size):\n",
    "        \n",
    "        if i in val_indices:\n",
    "            X_valid.append(data[i])\n",
    "            y_valid.append(labels[i])\n",
    "        else:\n",
    "            X_train.append(data[i])\n",
    "            y_train.append(labels[i])\n",
    "            \n",
    "    return np.array(X_train), np.array(y_train), np.array(X_valid), np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# -------------------\n",
    "#   Takes in pandas DataFrame and returns a numpy array that has all \n",
    "#   input features. Note that test data does not contain any review\n",
    "#   text, so no need to hot-encode any text.\n",
    "def process_test_data(data):     \n",
    "    unique_styles = list(beer_styles.keys())\n",
    "    features = []\n",
    "    \n",
    "    # Iterate over pandas DataFrame\n",
    "    for i, review in enumerate(data.iterrows()):\n",
    "        review = review[1]\n",
    "        \n",
    "        style = review['beer/style']\n",
    "        rating = review['review/overall']\n",
    "        \n",
    "        if style not in unique_styles:\n",
    "            style = max(beer_styles.items(), key=operator.itemgetter(1))[0]\n",
    "        \n",
    "        onehot_style = np.zeros(len(unique_styles))   # Initialize one-hot encoded\n",
    "        onehot_style[unique_styles.index(style)] = 1  # One-hot encode beer style\n",
    "        \n",
    "        print(\"---Process Testing---Index: \" + str(i) + \"; Percent complete: \" + \n",
    "              str(round((i/ (1.0 * len(data)) * 100), 2)) + \"%\")\n",
    "\n",
    "        features.append(np.array([onehot_style, rating]))\n",
    "        \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# -------------------\n",
    "#   This function appends each review in 'orig_data' with enough <PAD> \n",
    "#   characters s.t. each review is the same length as the longest\n",
    "#   review in the batch. Also, this function pads each review w/\n",
    "#   the <SOS> and <EOS> characters.\n",
    "#\n",
    "#   ** NOTE **\n",
    "#\n",
    "#   It is assumed that 'orig_data' is a numpy array of hot-encoded characters\n",
    "def pad_data(orig_data):\n",
    "    # Find longest review in batch\n",
    "    reviews = orig_data.tolist()\n",
    "    max_len = len(max(reviews, key=len))\n",
    "    \n",
    "    SOS_val = np.zeros(n_letters)  # [1, 0, 0 ... 0] represents SOS\n",
    "    SOS_val[0] = 1\n",
    "    \n",
    "    EOS_val = np.zeros(n_letters)  # [0, 0, 0 ... 1] represents EOS\n",
    "    EOS_val[n_letters - 1] = 1\n",
    "    \n",
    "    # Loop over all reviews in batch\n",
    "    for i in range(orig_data.shape[0]):\n",
    "      \n",
    "        # Pad with PAD character s.t. all reviews are of same length\n",
    "        padding = np.zeros(((max_len - orig_data[i].shape[0]), n_letters))\n",
    "        \n",
    "        # [0, 0, ... 1, 0] represents PAD\n",
    "        padding[:, (n_letters - 2)] = 1\n",
    "        \n",
    "        orig_data[i] = np.concatenate(([SOS_val], orig_data[i], \n",
    "                                       [EOS_val], padding), axis=0)\n",
    "    return orig_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_name, X_train, y_train, X_valid, y_valid, cfg, computing_device, \n",
    "          optimizer, criterion):\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    bleu_scores = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    min_valid_loss = 100000000\n",
    "    early_stop_count = 0\n",
    "       \n",
    "    for epoch in range(cfg['epochs']):\n",
    "        epoch_train_loss = []\n",
    "        \n",
    "        # ------------------- Training \n",
    "    \n",
    "        for minibatch_count in range(0, len(X_train), cfg['batch_size']):\n",
    "            \n",
    "            train_feats = X_train[minibatch_count:(minibatch_count+cfg['batch_size'])]\n",
    "            \n",
    "            train_labels  = y_train[minibatch_count:(minibatch_count+cfg['batch_size'])]    \n",
    "            onehot_labels = []\n",
    "            \n",
    "            # One-hot encode labels\n",
    "            for review_count in range(train_labels.shape[0]):\n",
    "                onehot_labels.append(text_to_onehot(train_labels[review_count][:cfg['max_train_len']]))\n",
    "            \n",
    "            # Pad labels with encoded <SOS> and <EOS> values\n",
    "            onehot_labels = np.array(onehot_labels)\n",
    "            onehot_labels = pad_data(onehot_labels)\n",
    "            \n",
    "            # Concatenate [ beer_style | beer_rating | char_encoding ] \n",
    "            # for forward pass\n",
    "            sequence, labels = batch_to_sequence(train_feats, onehot_labels)\n",
    "        \n",
    "                    \n",
    "            # Put minibatch data in CUDA tensors and run on GPU if supported\n",
    "            sequence, labels = sequence.float().to(computing_device), labels.to(computing_device)\n",
    "            \n",
    "            # Zero out the stored gradient (buffer) from the previous iteration\n",
    "            optimizer.zero_grad()\n",
    "            # Perform the forward pass through the network and compute the loss\n",
    "            outputs = model(sequence)\n",
    "            \n",
    "            loss = 0\n",
    "            for i in range(outputs.shape[1]):\n",
    "                ispadded = (labels[:,i] != (n_letters - 2))\n",
    "                \n",
    "                # Ignore the gradients for <PAD> characters\n",
    "                l = torch.mul(criterion(outputs[:,i], labels[:,i]), ispadded.float())\n",
    "                \n",
    "                # Divide by number of characters\n",
    "                divisor = torch.tensor(outputs.shape[1])\n",
    "                divisor = divisor.expand_as(l).float().to(computing_device)\n",
    "           \n",
    "                l = torch.div(l, divisor)\n",
    "                loss += torch.mean(l, 0)\n",
    "            \n",
    "            # Automagically compute the gradients and backpropagate the loss through the network\n",
    "            loss.backward()\n",
    "            epoch_train_loss.append(loss.cpu().item())\n",
    "            \n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            if minibatch_count % 100 == 0: \n",
    "                print(\"--- Epoch: \" + str(epoch) + \"; Minibatch: \" + str(minibatch_count) + \n",
    "                      \"; Avg Train Loss: \" + str(np.mean(np.array(epoch_train_loss))))\n",
    "                print(\"--- Original Review ---\")\n",
    "                print(train_labels[0])\n",
    "                print(\"--- Outputted Review ---\")\n",
    "                print(onehot_to_text(outputs[0]))\n",
    "            \n",
    "        train_loss.append(np.mean(np.array(epoch_train_loss)))\n",
    "        \n",
    "        # ------------------- Validation\n",
    "        epoch_valid_loss = []\n",
    "        epoch_bleu_score = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for minibatch_count in range(0, len(X_valid), cfg['batch_size']):\n",
    "\n",
    "                valid_feats = X_valid[minibatch_count:(minibatch_count+cfg['batch_size'])]\n",
    "\n",
    "                valid_labels  = y_valid[minibatch_count:(minibatch_count+cfg['batch_size'])]    \n",
    "                onehot_labels = []\n",
    "\n",
    "                # One-hot encode labels\n",
    "                for review_count in range(valid_labels.shape[0]):\n",
    "                    onehot_labels.append(text_to_onehot(valid_labels[review_count][:cfg['max_train_len']]))\n",
    "\n",
    "                # Pad labels with encoded <SOS> and <EOS> values\n",
    "                onehot_labels = np.array(onehot_labels)\n",
    "                onehot_labels = pad_data(onehot_labels)\n",
    "\n",
    "                # Concatenate [ beer_style | beer_rating | char_encoding ] \n",
    "                # for forward pass\n",
    "                sequence, labels = batch_to_sequence(valid_feats, onehot_labels)\n",
    "             \n",
    "                # Put minibatch data in CUDA tensors and run on GPU if supported\n",
    "                sequence, labels = sequence.float().to(computing_device), labels.to(computing_device)\n",
    "\n",
    "                # Perform the forward pass through the network and compute the loss\n",
    "                outputs = model(sequence)\n",
    "            \n",
    "                loss = 0\n",
    "                for i in range(outputs.shape[1]):\n",
    "                    ispadded = (labels[:,i] != (n_letters - 2))\n",
    "                    # Ignore the gradients for <PAD> characters\n",
    "                    l = torch.mul(criterion(outputs[:,i], labels[:,i]), ispadded.float())\n",
    "                    \n",
    "                    # Divide by number of characters\n",
    "                    divisor = torch.tensor(outputs.shape[1])\n",
    "                    divisor = divisor.expand_as(l).float().to(computing_device)\n",
    "           \n",
    "                    l = torch.div(l, divisor)\n",
    "                    loss += torch.mean(l, 0)\n",
    "\n",
    "                epoch_valid_loss.append(loss.cpu().item())\n",
    "        \n",
    "                # Calculate the batch bleu scores\n",
    "                batch_bleu_score = []\n",
    "                \n",
    "                for i in range(len(outputs)):                 \n",
    "                    reference  = valid_labels[i]\n",
    "                    hypothesis = onehot_to_text(outputs[i])\n",
    "                                        \n",
    "                    batch_bleu_score.append(calc_BLEUscore(reference, hypothesis))\n",
    "                epoch_bleu_score.append(np.mean(batch_bleu_score))\n",
    "        \n",
    "        epoch_bleu_score = np.mean(np.array(epoch_bleu_score))\n",
    "        \n",
    "        bleu_scores.append(epoch_bleu_score)     \n",
    "        valid_loss.append(np.mean(np.array(epoch_valid_loss))) \n",
    "        \n",
    "        epoch_loss = np.mean(np.array(epoch_valid_loss))\n",
    "        \n",
    "        print(\"--- Epoch: \" + str(epoch) + \"; Avg Valid Loss: \" + \n",
    "              str(epoch_loss))\n",
    "        \n",
    "        # Early stopping \n",
    "        if epoch_loss >= min_valid_loss:\n",
    "            early_stop_count += 1\n",
    "            \n",
    "            if early_stop_count == 3:\n",
    "                print(\"--- Early Stopping @ Epoch: \" + str(epoch) + \n",
    "                      \"; Min. Validation Loss: \" + str(min_valid_loss))\n",
    "                break\n",
    "        else:\n",
    "            early_stop_count = 0\n",
    "            min_valid_loss = epoch_loss\n",
    "            \n",
    "        print(\"Finished training on \" + str(epoch + 1) + \n",
    "              \" epoch; Bleu Score: \" + str(epoch_bleu_score) + \n",
    "              \"; Seconds Lapsed: \" + str(round((time.time() - start), 2)))\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "    print(\"Training complete after \" + str(epoch + 1) + \" epochs\")\n",
    "    print(\"Saving model\")\n",
    "    torch.save(model, model_name + \"_epoch\" + str(epoch + 1) + '.pt')\n",
    "    print(\"Save complete\")\n",
    "    \n",
    "    return train_loss, valid_loss, bleu_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(outputs, fname):\n",
    "    with open(fname, 'a') as out_file:\n",
    "        \n",
    "        for output in outputs:\n",
    "            text = onehot_to_text(output)\n",
    "            text = text.split('<EOS>')[0]\n",
    "            out_file.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, X_test, cfg, computing_device):\n",
    "    # TODO: Given n rows in test data, generate a list of n strings, where each string is the review\n",
    "    # corresponding to each input row in test data.\n",
    "    reviews = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for minibatch_count in range(0, len(X_test), cfg['batch_size']):\n",
    "            # Concatenate [ beer_style | beer_rating | char_encoding ] \n",
    "            # for forward pass\n",
    "            feats = X_test[minibatch_count:(minibatch_count+cfg['batch_size'])]\n",
    "            \n",
    "            metadata = []\n",
    "            \n",
    "            for i in range(len(feats)):\n",
    "                style = feats[i][0]\n",
    "                rating = feats[i][1]\n",
    "                \n",
    "                metadata.append(torch.from_numpy(\n",
    "                    np.concatenate((style, np.array([rating])))))\n",
    "                \n",
    "            metadata = torch.stack(metadata, dim=0) \n",
    "            # Put minibatch data in CUDA tensors and run on GPU if supported\n",
    "            metadata = metadata.float().to(computing_device)\n",
    "\n",
    "            # Perform the forward pass through the network and compute the loss\n",
    "            outputs = model.generate(cfg, metadata)\n",
    "            reviews.append(outputs)\n",
    "            \n",
    "            # Output minibatch to file\n",
    "            save_to_file(outputs, \"reviews_tau_\" + str(cfg['gen_temp']) + \".txt\")\n",
    "            \n",
    "            if minibatch_count % 100 == 0: \n",
    "                print(\"--- Minibatch: \" + str(minibatch_count))\n",
    "                print(\"--- Generated Review ---\")\n",
    "                print(onehot_to_text(outputs[0]))\n",
    "    \n",
    "    reviews = np.array(reviews)\n",
    "    \n",
    "    return reviews.flatten.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    # Converting DataFrame to numpy array\n",
    "    train_data, train_labels = process_train_data(train_data) \n",
    "    # Splitting the train data into train-valid data\n",
    "    X_train, y_train, X_valid, y_valid = train_valid_split(train_data, train_labels) \n",
    "    \n",
    "    X_test = process_test_data(test_data)    # Converting DataFrame to numpy array\n",
    "    \n",
    "    model = baselineLSTM(cfg) # Replace this with model = <your model name>(cfg)\n",
    "    if cfg['cuda']:\n",
    "        computing_device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        computing_device = torch.device(\"cpu\")\n",
    "    model.to(computing_device)\n",
    "    \n",
    "    train(model, X_train, y_train, X_valid, y_valid, cfg) # Train the model\n",
    "    \n",
    "    outputs = generate(model, X_test, cfg) # Generate the outputs for test data\n",
    "    save_to_file(outputs, out_fname)       # Save the generated outputs to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_fname = \"Beeradvocate_Train.csv\"\n",
    "test_data_fname = \"Beeradvocate_Test.csv\"\n",
    "out_fname = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(train_data_fname) # Generating the pandas DataFrame\n",
    "test_data = load_data(test_data_fname)   # Generating the pandas DataFrame\n",
    "\n",
    "print(len(train_data.index))\n",
    "print(len(test_data.index))\n",
    "\n",
    "train_data.replace(\"\", np.nan, inplace=True)\n",
    "train_data.dropna(inplace=True)\n",
    "\n",
    "# Cut-down data for runtime purposes\n",
    "train_data = train_data[:4800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all unique beer styles\n",
    "beer_types = sorted(train_data['beer/style'].unique(), key=str.lower)\n",
    "temp = dict.fromkeys(beer_types)\n",
    "\n",
    "for key in temp:\n",
    "    temp[key] = train_data[train_data['beer/style'] == key].shape[0]\n",
    "\n",
    "beer_styles = temp\n",
    "print(list(beer_styles.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting DataFrame to numpy array\n",
    "train_feats, train_labels = process_train_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the train data into train-valid data\n",
    "X_train, y_train, X_valid, y_valid = train_valid_split(train_feats, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting DataFrame to numpy array\n",
    "X_test = process_test_data(test_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg['cuda']:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is supported\")\n",
    "else:\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    print(\"CUDA NOT supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Train LSTM and GRU on 1st set of hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {}\n",
    "\n",
    "hyperparams['dropout'] = [0.0, 0.1]\n",
    "hyperparams['hidden_dim'] = [64, 32]\n",
    "hyperparams['learning_rate'] = [0.005, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['dropout'] = hyperparams['dropout'][0]\n",
    "cfg['hidden_dim'] = hyperparams['hidden_dim'][0]\n",
    "cfg['learning_rate'] = hyperparams['learning_rate'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1 = myLSTM(cfg)\n",
    "lstm1.to(computing_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru1 = myGRU(cfg)\n",
    "gru1.to(computing_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM1optimizer = optim.Adam(lstm1.parameters(), lr=cfg['learning_rate'])\n",
    "GRU1optimizer = optim.Adam(gru1.parameters(), lr=cfg['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "lstm1_train_losses, lstm1_valid_losses, lstm1_bleu_scores = train(\n",
    "    lstm1, \"LSTM_model1\", X_train, y_train, X_valid, y_valid, cfg, \n",
    "    computing_device, LSTM1optimizer, criterion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lstm1_train_losses)\n",
    "print()\n",
    "print(lstm1_valid_losses)\n",
    "print()\n",
    "print(lstm1_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(lstm1_train_losses)), lstm1_train_losses, 'b--', label='Training Loss')\n",
    "plt.plot(range(len(lstm1_valid_losses)), lstm1_valid_losses, 'r--', label='Validation Loss')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"LSTM Model 1: Training vs. Validation Losses\")\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.savefig(\"images/lstm1_losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(lstm1_bleu_scores)), lstm1_bleu_scores, 'b--', label='Bleu Scores')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Bleu Score\")\n",
    "plt.title(\"LSTM Model 1: Bleu Scores on Validation Set\")\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.savefig(\"images/lstm1_bleus.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "gru1_train_losses, gru1_valid_losses, gru1_bleu_scores = train(\n",
    "    gru1, \"GRU_model1\", X_train, y_train, X_valid, y_valid, cfg, \n",
    "    computing_device, GRU1optimizer, criterion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gru1_train_losses)\n",
    "print()\n",
    "print(gru1_valid_losses)\n",
    "print()\n",
    "print(gru1_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(gru1_train_losses)), gru1_train_losses, 'b--', label='Training Loss')\n",
    "plt.plot(range(len(gru1_valid_losses)), gru1_valid_losses, 'r--', label='Validation Loss')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"GRU Model 1: Training vs. Validation Losses\")\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.savefig(\"images/gru1_losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(gru1_bleu_scores)), gru1_bleu_scores, 'b--', label='Bleu Scores')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Bleu Score\")\n",
    "plt.title(\"GRU Model 1: Bleu Scores on Validation Set\")\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.savefig(\"images/gru1_bleus.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Train LSTM and GRU on 2nd set of hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['dropout'] = hyperparams['dropout'][1]\n",
    "cfg['hidden_dim'] = hyperparams['hidden_dim'][1]\n",
    "cfg['learning_rate'] = hyperparams['learning_rate'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm2 = myLSTM(cfg)\n",
    "lstm2.to(computing_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['dropout'] = hyperparams['dropout'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru2 = myGRU(cfg)\n",
    "gru2.to(computing_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM2optimizer = optim.SGD(lstm2.parameters(), lr=cfg['learning_rate'])\n",
    "GRU2optimizer = optim.SGD(gru2.parameters(), lr=cfg['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "lstm2_train_losses, lstm2_valid_losses, lstm2_bleu_scores = train(\n",
    "    lstm2, \"LSTM_model2\", X_train, y_train, X_valid, y_valid, cfg, \n",
    "    computing_device, LSTM2optimizer, criterion)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lstm2_train_losses)\n",
    "print()\n",
    "print(lstm2_valid_losses)\n",
    "print()\n",
    "print(lstm2_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(lstm2_train_losses)), lstm2_train_losses, 'b--', label='Training Loss')\n",
    "plt.plot(range(len(lstm2_valid_losses)), lstm2_valid_losses, 'r--', label='Validation Loss')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"LSTM Model 2: Training vs. Validation Losses\")\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.savefig(\"images/lstm2_losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(lstm2_bleu_scores)), lstm2_bleu_scores, 'b--', label='Bleu Scores')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Bleu Score\")\n",
    "plt.title(\"LSTM Model 2: Bleu Scores on Validation Set\")\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.savefig(\"images/lstm2_bleus.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['dropout'] = hyperparams['dropout'][0]\n",
    "\n",
    "gru2 = myGRU(cfg)\n",
    "gru2.to(computing_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "gru2_train_losses, gru2_valid_losses, gru2_bleu_scores = train(\n",
    "    gru2, \"GRU_model2\", X_train, y_train, X_valid, y_valid, cfg, \n",
    "    computing_device, GRU2optimizer, criterion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gru2_train_losses)\n",
    "print()\n",
    "print(gru2_valid_losses)\n",
    "print()\n",
    "print(gru2_bleu_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(gru2_train_losses)), gru2_train_losses, 'b--', label='Training Loss')\n",
    "plt.plot(range(len(gru2_valid_losses)), gru2_valid_losses, 'r--', label='Validation Loss')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"GRU Model 2: Training vs. Validation Losses\")\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.savefig(\"images/gru2_losses.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(gru2_bleu_scores)), gru2_bleu_scores, 'b--', label='Bleu Scores')\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Bleu Score\")\n",
    "plt.title(\"GRU Model 2: Bleu Scores on Validation Set\")\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.savefig(\"images/gru2_bleus.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Generate reviews using best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1_avg_bleu = np.mean(lstm1_bleu_scores)\n",
    "lstm2_avg_bleu = np.mean(lstm2_bleu_scores)\n",
    "gru1_avg_bleu = np.mean(gru1_bleu_scores)\n",
    "gru2_avg_bleu = np.mean(gru2_bleu_scores)\n",
    "\n",
    "avg_bleu_scores = {lstm1: lstm1_avg_bleu, lstm2: lstm2_avg_bleu, \n",
    "                   gru1: gru1_avg_bleu, gru2: gru2_avg_bleu}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = max(avg_bleu_scores.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "print(\"Saving BEST model: \" + str(best_model))\n",
    "torch.save(best_model, 'best_model.pt')\n",
    "print(\"Save complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_bleu_scores[best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('GRU_model1_epoch15.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0.4, 0.01, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(temperatures)):\n",
    "    cfg['gen_temp'] = temperatures[t]\n",
    "    generate(best_model, X_test, cfg, computing_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
